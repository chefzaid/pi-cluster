# Prometheus + Grafana monitoring stack for K3s Raspberry Pi cluster
# Monitors: CPU, RAM, disk, network, temperature, and Kubernetes metrics
# Uses node-exporter DaemonSet on all nodes for hardware/OS metrics
---
apiVersion: v1
kind: Namespace
metadata:
  name: monitoring
---
# ============================================================
#  Prometheus - cluster-wide metrics collection
# ============================================================
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: monitoring
data:
  prometheus.yml: |
    global:
      scrape_interval: 60s          # 60s is plenty for a Pi4 cluster; saves CPU
      evaluation_interval: 60s

    scrape_configs:
      # --- Prometheus self-monitoring ---
      - job_name: prometheus
        static_configs:
          - targets: ["localhost:9090"]

      # --- Node Exporter (CPU, RAM, disk, network, temperature) ---
      - job_name: node-exporter
        kubernetes_sd_configs:
          - role: endpoints
            namespaces:
              names: ["monitoring"]
        relabel_configs:
          - source_labels: [__meta_kubernetes_endpoints_name]
            regex: node-exporter-service
            action: keep
          - source_labels: [__meta_kubernetes_endpoint_node_name]
            target_label: node

      # --- Kubelet / cAdvisor (container metrics) ---
      - job_name: kubelet
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          insecure_skip_verify: true
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        kubernetes_sd_configs:
          - role: node
        relabel_configs:
          - target_label: __address__
            replacement: kubernetes.default.svc:443
          - source_labels: [__meta_kubernetes_node_name]
            target_label: __metrics_path__
            replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor
          - source_labels: [__meta_kubernetes_node_name]
            target_label: node

      # --- Kube API server ---
      - job_name: kubernetes-apiservers
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          insecure_skip_verify: true
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        kubernetes_sd_configs:
          - role: endpoints
        relabel_configs:
          - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
            regex: default;kubernetes;https
            action: keep
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: prometheus
  namespace: monitoring
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: prometheus
rules:
  - apiGroups: [""]
    resources: [nodes, nodes/proxy, nodes/metrics, services, endpoints, pods]
    verbs: [get, list, watch]
  - apiGroups: ["extensions", "networking.k8s.io"]
    resources: [ingresses]
    verbs: [get, list, watch]
  - nonResourceURLs: ["/metrics", "/metrics/cadvisor"]
    verbs: [get]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: prometheus
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: prometheus
subjects:
  - kind: ServiceAccount
    name: prometheus
    namespace: monitoring
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: prometheus-pvc
  namespace: monitoring
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: longhorn
  resources:
    requests:
      storage: 5Gi
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: prometheus
  namespace: monitoring
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: prometheus
  template:
    metadata:
      labels:
        app: prometheus
    spec:
      serviceAccountName: prometheus
      securityContext:
        fsGroup: 65534
        runAsUser: 65534
        runAsNonRoot: true
      containers:
        - name: prometheus
          image: prom/prometheus:v3.2.1
          args:
            - "--config.file=/etc/prometheus/prometheus.yml"
            - "--storage.tsdb.path=/prometheus"
            - "--storage.tsdb.retention.time=15d"
            - "--storage.tsdb.retention.size=4GB"
            - "--web.enable-lifecycle"
          ports:
            - containerPort: 9090
          volumeMounts:
            - name: config
              mountPath: /etc/prometheus
            - name: data
              mountPath: /prometheus
          resources:
            requests:
              memory: "128Mi"
              cpu: "50m"
            limits:
              memory: "384Mi"
              cpu: "300m"
          readinessProbe:
            httpGet:
              path: /-/ready
              port: 9090
            initialDelaySeconds: 15
            periodSeconds: 10
          livenessProbe:
            httpGet:
              path: /-/healthy
              port: 9090
            initialDelaySeconds: 30
            periodSeconds: 30
      volumes:
        - name: config
          configMap:
            name: prometheus-config
        - name: data
          persistentVolumeClaim:
            claimName: prometheus-pvc
---
apiVersion: v1
kind: Service
metadata:
  name: prometheus-service
  namespace: monitoring
spec:
  selector:
    app: prometheus
  ports:
    - protocol: TCP
      port: 9090
      targetPort: 9090
---
# ============================================================
#  Node Exporter - DaemonSet on every node
#  Exposes CPU, RAM, disk, network, filesystem, temperature
# ============================================================
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: node-exporter
  namespace: monitoring
spec:
  selector:
    matchLabels:
      app: node-exporter
  template:
    metadata:
      labels:
        app: node-exporter
    spec:
      automountServiceAccountToken: false
      hostPID: true
      hostNetwork: true
      dnsPolicy: ClusterFirstWithHostNet
      tolerations:
        - operator: Exists    # run on ALL nodes including control plane
      containers:
        - name: node-exporter
          image: prom/node-exporter:v1.9.0
          args:
            - "--path.rootfs=/host"
            - "--collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)"
            - "--collector.hwmon"          # Pi4 CPU temperature via /sys/class/hwmon
            - "--collector.cpu"
            - "--collector.meminfo"
            - "--collector.diskstats"
            - "--collector.netdev"
            - "--collector.loadavg"
            - "--collector.thermal_zone"   # thermal zone temperatures
          ports:
            - containerPort: 9100
              hostPort: 9100
          volumeMounts:
            - name: rootfs
              mountPath: /host
              readOnly: true
              mountPropagation: HostToContainer
          resources:
            requests:
              memory: "32Mi"
              cpu: "25m"
            limits:
              memory: "64Mi"
              cpu: "100m"
      volumes:
        - name: rootfs
          hostPath:
            path: /
---
apiVersion: v1
kind: Service
metadata:
  name: node-exporter-service
  namespace: monitoring
spec:
  clusterIP: None    # headless - Prometheus discovers endpoints via SD
  selector:
    app: node-exporter
  ports:
    - protocol: TCP
      port: 9100
      targetPort: 9100
---
# ============================================================
#  Grafana - dashboards & visualization
# ============================================================
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: grafana-pvc
  namespace: monitoring
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: longhorn
  resources:
    requests:
      storage: 1Gi
---
# Datasource provisioning - auto-connect Prometheus
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-datasources
  namespace: monitoring
data:
  datasources.yaml: |
    apiVersion: 1
    datasources:
      - name: Prometheus
        type: prometheus
        uid: prometheus
        access: proxy
        url: http://prometheus-service.monitoring.svc.cluster.local:9090
        isDefault: true
        editable: false
---
# Dashboard provisioning config
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-dashboard-providers
  namespace: monitoring
data:
  dashboards.yaml: |
    apiVersion: 1
    providers:
      - name: default
        orgId: 1
        folder: ""
        type: file
        disableDeletion: false
        editable: true
        options:
          path: /var/lib/grafana/dashboards
          foldersFromFilesStructure: false
---
# Pre-loaded Node Exporter Full dashboard (Grafana dashboard ID 1860, adapted for Pi)
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-dashboards
  namespace: monitoring
data:
  pi-cluster-overview.json: |
    {
      "annotations": { "list": [] },
      "editable": true,
      "fiscalYearStartMonth": 0,
      "graphTooltip": 1,
      "links": [],
      "panels": [
        {
          "title": "CPU Usage per Node",
          "type": "timeseries",
          "datasource": { "type": "prometheus", "uid": "prometheus" },
          "gridPos": { "h": 8, "w": 12, "x": 0, "y": 0 },
          "targets": [{
            "expr": "100 - (avg by(node) (rate(node_cpu_seconds_total{mode=\"idle\"}[5m])) * 100)",
            "legendFormat": "{{ node }}"
          }],
          "fieldConfig": {
            "defaults": { "unit": "percent", "min": 0, "max": 100 },
            "overrides": []
          }
        },
        {
          "title": "Memory Usage per Node",
          "type": "timeseries",
          "datasource": { "type": "prometheus", "uid": "prometheus" },
          "gridPos": { "h": 8, "w": 12, "x": 12, "y": 0 },
          "targets": [{
            "expr": "100 * (1 - node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)",
            "legendFormat": "{{ node }}"
          }],
          "fieldConfig": {
            "defaults": { "unit": "percent", "min": 0, "max": 100 },
            "overrides": []
          }
        },
        {
          "title": "CPU Temperature per Node",
          "type": "timeseries",
          "datasource": { "type": "prometheus", "uid": "prometheus" },
          "gridPos": { "h": 8, "w": 12, "x": 0, "y": 8 },
          "targets": [{
            "expr": "node_hwmon_temp_celsius",
            "legendFormat": "{{ node }}"
          },
          {
            "expr": "node_thermal_zone_temp",
            "legendFormat": "{{ node }} (thermal_zone)"
          }],
          "fieldConfig": {
            "defaults": { "unit": "celsius", "min": 20, "max": 90 },
            "overrides": []
          }
        },
        {
          "title": "Disk Usage per Node",
          "type": "timeseries",
          "datasource": { "type": "prometheus", "uid": "prometheus" },
          "gridPos": { "h": 8, "w": 12, "x": 12, "y": 8 },
          "targets": [{
            "expr": "100 - (node_filesystem_avail_bytes{mountpoint=\"/\",fstype!=\"tmpfs\"} / node_filesystem_size_bytes{mountpoint=\"/\",fstype!=\"tmpfs\"} * 100)",
            "legendFormat": "{{ node }}"
          }],
          "fieldConfig": {
            "defaults": { "unit": "percent", "min": 0, "max": 100 },
            "overrides": []
          }
        },
        {
          "title": "Network Receive per Node",
          "type": "timeseries",
          "datasource": { "type": "prometheus", "uid": "prometheus" },
          "gridPos": { "h": 8, "w": 12, "x": 0, "y": 16 },
          "targets": [{
            "expr": "rate(node_network_receive_bytes_total{device=~\"eth.*|end.*\"}[5m])",
            "legendFormat": "{{ node }} - {{ device }}"
          }],
          "fieldConfig": {
            "defaults": { "unit": "Bps" },
            "overrides": []
          }
        },
        {
          "title": "Network Transmit per Node",
          "type": "timeseries",
          "datasource": { "type": "prometheus", "uid": "prometheus" },
          "gridPos": { "h": 8, "w": 12, "x": 12, "y": 16 },
          "targets": [{
            "expr": "rate(node_network_transmit_bytes_total{device=~\"eth.*|end.*\"}[5m])",
            "legendFormat": "{{ node }} - {{ device }}"
          }],
          "fieldConfig": {
            "defaults": { "unit": "Bps" },
            "overrides": []
          }
        },
        {
          "title": "System Load (1m) per Node",
          "type": "timeseries",
          "datasource": { "type": "prometheus", "uid": "prometheus" },
          "gridPos": { "h": 8, "w": 12, "x": 0, "y": 24 },
          "targets": [{
            "expr": "node_load1",
            "legendFormat": "{{ node }}"
          }],
          "fieldConfig": {
            "defaults": { "unit": "short" },
            "overrides": []
          }
        },
        {
          "title": "Disk I/O per Node",
          "type": "timeseries",
          "datasource": { "type": "prometheus", "uid": "prometheus" },
          "gridPos": { "h": 8, "w": 12, "x": 12, "y": 24 },
          "targets": [{
            "expr": "rate(node_disk_read_bytes_total{device=~\"mmcblk.*|sd.*\"}[5m])",
            "legendFormat": "{{ node }} read"
          },
          {
            "expr": "rate(node_disk_written_bytes_total{device=~\"mmcblk.*|sd.*\"}[5m])",
            "legendFormat": "{{ node }} write"
          }],
          "fieldConfig": {
            "defaults": { "unit": "Bps" },
            "overrides": []
          }
        }
      ],
      "schemaVersion": 39,
      "tags": ["pi-cluster", "nodes"],
      "templating": { "list": [] },
      "time": { "from": "now-1h", "to": "now" },
      "title": "Pi Cluster Overview",
      "uid": "pi-cluster-overview"
    }
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: grafana
  namespace: monitoring
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: grafana
  template:
    metadata:
      labels:
        app: grafana
    spec:
      automountServiceAccountToken: false
      securityContext:
        fsGroup: 472
        runAsUser: 472
        runAsNonRoot: true
      containers:
        - name: grafana
          image: grafana/grafana:11.5.2
          ports:
            - containerPort: 3000
          env:
            - name: GF_SECURITY_ADMIN_USER
              value: admin
            - name: GF_SECURITY_ADMIN_PASSWORD
              value: admin                  # CHANGE ON FIRST LOGIN - or replace with a K8s Secret
            - name: GF_DASHBOARDS_DEFAULT_HOME_DASHBOARD_PATH
              value: /var/lib/grafana/dashboards/pi-cluster-overview.json
            - name: GF_INSTALL_PLUGINS
              value: ""
          volumeMounts:
            - name: data
              mountPath: /var/lib/grafana
            - name: datasources
              mountPath: /etc/grafana/provisioning/datasources
            - name: dashboard-providers
              mountPath: /etc/grafana/provisioning/dashboards
            - name: dashboards
              mountPath: /var/lib/grafana/dashboards
          resources:
            requests:
              memory: "128Mi"
              cpu: "50m"
            limits:
              memory: "256Mi"
              cpu: "250m"
          readinessProbe:
            httpGet:
              path: /api/health
              port: 3000
            initialDelaySeconds: 15
            periodSeconds: 10
          livenessProbe:
            httpGet:
              path: /api/health
              port: 3000
            initialDelaySeconds: 30
            periodSeconds: 30
      volumes:
        - name: data
          persistentVolumeClaim:
            claimName: grafana-pvc
        - name: datasources
          configMap:
            name: grafana-datasources
        - name: dashboard-providers
          configMap:
            name: grafana-dashboard-providers
        - name: dashboards
          configMap:
            name: grafana-dashboards
---
apiVersion: v1
kind: Service
metadata:
  name: grafana-service
  namespace: monitoring
spec:
  selector:
    app: grafana
  ports:
    - protocol: TCP
      port: 80
      targetPort: 3000
---
# ============================================================
#  Traefik Ingress - access Grafana at grafana.local
#  (same pattern as longhorn.local - resolved by Traefik/K3s)
# ============================================================
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: grafana-ingress
  namespace: monitoring
  annotations:
    traefik.ingress.kubernetes.io/router.entrypoints: web
spec:
  rules:
    - host: grafana.local
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: grafana-service
                port:
                  number: 80
